\section{Literature Review}

\subsection{Disinformation Research}
%briefly: What is disinfo, who produces it. What is it called in lit (i.e. conspiracy theories, fake news)
Disinformation can be defined as false information created with the intent to mislead \citep{Fallis2015}.  Commonly associated terms in literature are misinformation, conspiracy theories, fake news, and hoaxes. Actors involved in the spread of disinformation range from bots and trolls to partisan media platforms and (national) government bodies \citep{Tucker2018}. The goals of those involved in the spread of disinformation are also varied, ranging from political influence \citep{keller2020political}, distracting the public \citep{king2017chinese}, to monetary gain and fun, amongst others  \citep{hrvckova2019unravelling}. \\ Online disinformation campaigns use the digital infrastructure provided by third party social media platforms to spread or advertise its content. As a result, these social media platforms have a mediating role between disinformation campaigns and their target(s) \citep{Bessi2015}. As a result, (mainly) US tech companies hold a lot of power in this domain. Additionally, messaging boards like reddit or 4chan are used as testing or breeding grounds of disinformation campaigns \citep{Morgan2018}.  \\

In this section I provide a brief overview of disinformation research based on three perspectives:
\begin{itemize}
	\item \textbf{Computer Science:} The focus is on the detection of properties of disinformation, either of content itself (feature detection) or particular patterns of its engagement (i.e. likes, lifespan of interaction, spread over a network). This perspective asks \textit{``What does it look like?''}
	\item \textbf{Psychology:} Aims at understanding the psychological traits or mechanism that make individuals vulnerable to disinformation. This perspective asks \textit{``Why does it work?''}
	\item \textbf{Societal:} Discusses the effects disinformation has on a societal level, i.e. on politics, polarization, and behaviour. This perspective asks \textit{``What does it do?''}
\end{itemize}

\subsubsection{Computer Science Perspective - What does disinformation look like?}
The computer science perspective is motivated by the underlying assumption that if disinformation can be detected automatically it can be removed, surpressed or labeled before it reaches a large audience. One set of studies tries to identify features that allow them to predict whether a piece of content is false (or a rumour) or not. Such features may be content-based, network-based or platform specific  \citep{qazvinian2011rumor}, or based on the user who posted the content \citep{liu2015real}. Machine-learning approaches are also used to train algorithms on large annotated datasets (i.e. \cite{wang2017liar}, \cite{mitra2015credbank} \cite{papadopoulou2019corpus}) yet these approaches are labour-intensive and subsequent algorithms may be easily fooled and perform well only on the datasets they were trained on \citep{grondahl2018all}. \\

When studying how users engage with misinformation, \cite{Zollo2018} claim that the spread of misinformation is driven by confirmation bias and enabled by echo chambers that are formed on the web. The spread of the disinformation is therefore determined by the size of the echo chambers. \cite{DelVicario2016} also find that echo chambers play a role and find that the cascade behaviour of conspiracy theories is different than that of science stories.  \cite{Kumar2018} also see a role played by echo chambers, suggesting that the impact of disinformation is high when individuals see the same message repeated over and over, as would occur in echo chambers. However, in their review, \cite{Tucker2018} question the claim that echo-chambers play a central role, as other research has shown that users are exposed to a variety of viewpoints on online media. \cite{Vosoughi2018} consider only one type of content (news) and note that false news stories spread faster and further, and suggest this might be because they are more novel. They also find that bots spread both true and false news at similar rates, suggesting that the reach of false news is due to human behaviour. \cite{Budak2011} propose that looking at dynamics of spread can be the solution to stopping the spread of disinformation: if influential users can be “infected” with true information early on, this can halt the campaign. \\



\subsubsection{Psychological perspective - Why does disinformation work?}
Reviewing relevant literature, \cite{Kumar2018} provide three reasons why people are vulnerable to disinformation: People are unable to tell if information is false, particularly if it has been well-crafted, echo-chambers result in frequent exposure which increases belief, and mechanisms like confirmation bias, naïve realism and social norms lead people to seek confirmation of their own beliefs and the acceptance of peers. 

When it comes to an individual's ability to detect disinformation, \cite{Pennycook2018} found that people who have high ``bullshit receptivity'' and lower analytical thinking skills are more likely to perceive fake news as being accurate.\cite{uscinski2020people} Studied belief in COVID-19 related conspiracies, and found that belief in these was predicted by denialism, conspiracy thinking, and ideological motivations (i.e. Trump supporters were more likely to believe these conspiracies).

Related to the issue of echo-chambers and repeated information, \cite{Pennycook2018a}) found that the illusionary truth effect (statements seem more plausible when they are repeated) held for political disinformation, with a significant but small increase in perceived accuracy of fake news which participants had been exposed to previously. Notably, they showed that warnings that content was false did not decrease this effect. Closely related is the “continued influence effect” where people continue to rely on information they know to be false. Warnings and alternative explanations have been shown to reduce this effect, but cannot remove it completely \cite{Ecker2010}.


Two cognitive biases that likely play a role in the effect of disinformation are anchoring and confirmation bias. Anchoring is the process where the first data one receives strongly influences any future beliefs or estimates about the truth. \cite{jost2020fake} showed that this also occurs in the context of fake news, even when people are aware the information they receive is false Confirmation bias occurs when people are (unconsciouly) motivated to believe certain statements and reject others based on their previous beliefs. \cite{taber2006motivated} showed that in a political context, people are much more likely to accept arguments that support their previous beliefs and will pick apart counter-arguments. A similar process likely occurs with disinformation.

Little is know as of yet how these factors (personal skills, biases) are different across different contexts. \cite{Bessi2015} showed that on Facebook, people who were involved in conspiracy theories on one topic were more likely to be involved in other conspiracy theories as well. There is also some evidence that personal involvement increases the willingness of people to spread rumors, especially if they induce fear \citep{chua2018intentions}, suggesting that besides the topic, the ``stakes'' associated with disinformation can also influence how it is processed.



\subsubsection{Societal perspective - What are the effects of disinformation?}
Research on the subsequent effect of this exposure to the individual is limited. Based previous research on the effectiveness of political ads, \cite{Allcott2017}) claim that the exposure of disinformation based on a dataset of fake news about the 2016 US election had a negligible effect because the average American was only exposed to a handful of false stories. \cite{Guess2020} found that consumption of fake news was linked to distrust towards media and stronger feelings of polarisation. In additional experiments, they showed that a single exposure to a false story would increase the belief of the participant in the (political) claim made in that article. Though this change was significant it was also small, so it is not yet clear if there is a meaningful effect. \\ 

Disinformation may also work in less direct ways. For example, people may have wrong perceptions about public policy or events, and these may be encouraged by disinformation (i.e. exaggerating the money spent on welfare could be beneficial to the agenda of the Republican party.) \citep{Tucker2018}. \cite{Kolmes2011} argues that a lack of belief in man-made global warming in the American public was the result of disinformation that was deliberately spread by fossil fuel companies, but such a level of influence would be extremely hard to quantify. Indeed, the broader the perspective and the less defined the case, the harder it becomes to describe the effects of disinformation using emperical data. However, even if an effect is not immediately tangible, it does not mean it is absent.
\begin{comment}
if I wanted to go really wild I could talk about hyperobjects here but probably not the best place. Would be a nice concept to use later on though.
\end{comment}
\cite{Asmolov2018} believes that disinformation is effective not because of its actual content, but because the arguments it creates sever social ties, resulting in a polarized society. However, the direction of causality is not clear - some argue that the prevalence of echo chambers is exaggerated, and that polarization precedes large-scale online disinformation campaigns, which then make use of resulting biases (and possibly exaggerate them) \citep{Tucker2018}. With a broader perspective, the possible sollutions to the problem of disinformation also change. \cite{lewandowsky2017beyond} reject that with better communication techniques the issue of disinformation can be solved. In their work about the ``Post-Truth Era'' they identify a number of trends that they believe have contributed to the issue, ranging form a decrease in trust, growing inequality, polarization, and an (online) media landscape that rewards extremism. They state \textit{``...post-truth claims [...] do not seek to establish a coherent model of reality. Rather, they erode trust in facts and reality, to the point where facts no longer matter or are not even acknowledged to exist.''} \\

Based on this brief review, we can make a number of observations. The first is that efforts aimed at detecting and removing disinformation would need to focus on doing so almost immediately once a piece of disinformation is introduced into an online environment, since the psychological perspective suggests that warnings or removal at a later stage would be unsuccesful. This is already challenging, since actors who deliberately spread disinformation can change their tactics to avoid detection. But even assuming that detection is feasible, mass deletion of posts might have other risks. Given that individuals who are most likely to believe in disinformation are also distrusting towards media and authority, this could also create an effect where these groups move to other (less-controlled) platforms. 
Another observation is that polarization and echo-chambers are likely related to the succes of disinformation, but the direction and srength of this relationship remains up for debate. This is closely related to the outstanding issue of long-term effects of disinformation campaigns, which are hard to study emperically and occur in a wide context of social, politcal and economical factors that may all determine the effect of disinformation. The fact that these mechanisms remain unclear, combined with the current knowledge on what individuals are vulnerable to disinformation, suggest that a broader, societal perspective on disinformation is needed.


\subsection{Network Science}
\subsubsection{Why is network science relevant to disinformation?}
Online disinformation raises concern because it can be spread fast and far. This is due to two reasons: 1) Individuals can (unknowingly) act as amplefiers of content in their social circle, by easily sharing content with their connections or by drawing attention to it, and 2) the physical and digital infrastructure of the internet enables and encourages a number and span of connections that was not possible before. In this sense, online disinformation campaigns harness the social and digital infrastructure of the internet to their benefit. Both the social and digital infrastrucutres are \textit{networks}, which motivates follwing discussion of network science. Additionally, network science has long been used to study topics closely aligned with the problem of disinformation, such as the diffusion of information over a population and the rise (and fall) of political movements \citep{Guilbeault2018}. Since In the network science approach, a system is modelled as a collection of nodes and edges. Nodes are the object of interests (such as individuals or computers) and the edges indicate a connection between the nodes. The underlying assumption of network science is that these connections are fundamental in understanding the system of interest as a whole \citep{Brandes2013}.  \\

A key feature of many real-world networks is that they are not random – which means that there is something driving the formation of networks, and in turn, the formation of a network can be used to achieve certain aims \citep{Newman2003}. An important discovery in network science was that of the small-world model by \cite{Watts1998}, who showed that this network structure occurs in many real-world networks and demonstrated that diseases spread much faster over these networks. Small-world networks can mathematically be described as having a mean shortest distance between nodes that scales logarithmically or slower with the number of nodes N \citep{Newman2003}. 
\cite{Barabasi1999} found many real-world networks to have a degree distribution that followed a power-law distribution and called these networks scale-free networks. They showed that these types of network are created if networks are grown following preferential attachment: new nodes attach to nodes which already have a high number of edges.


\subsubsection{Information Diffusion, (Complex) Contagion and Network Strucutres}
As shown above, several different types of networks exist, influenced by the contexts in which they grew. Nowadays, many of the networks used daily are not grown organically – they are at least partly designed. Therefore, it is relevant to consider if design choices influence information diffusion – a highly relevant process when studying disinformation. I consider two types of network characteristics. Structural characteristics relate to the network properties discussed above, such as the distribution of node degrees and the average path length. Node characteristics relate to what behaviour a node can perform, i.e. how many neighbours it can communicate to at one time, the maximum number of connections it can maintain, or when it will form or break a connection. \\

The structure of a network has effects on how information, or a message, is propagated over it. \cite{Bampo2008} studied the effect of network topology on the success of a viral marketing campaign by simulating it over a random, small-world and scale-free network. The scale-free network was more sensitive to changes in the seed size and the average number of connections a message was passed on to.  \\

Some insight on the influence of node attributes can be gained from a marketing study, which showed that the spread of a Facebook application differed significantly depending on whether users could personally invite their friends, or if the application sent out passive notifications \cite{Aral2011}. Though personal invitations had a higher adoption rate, passive notifications were sent far more often and as a result, were more effective. Therefore, how a node can behave in a network influences diffusion. This also concerns how a node became active in the diffusion process in the first place, or how contagion works. \\

Long before network science appeared as a stand-alone discipline, sociologist \cite{Granovetter1977} showed that weak ties – edges that connect individuals who otherwise have little in common – greatly increase the reach and speed of information spreading over a network. This finding was later confirmed by the small-world model of \cite{Watts1998}. This so called “Strength of Weak Ties” explains why information and diseases can spread rapidly across the world – one edge is enough to link a group that is otherwise isolated from another. \cite{Centola2007} argued, however, that the strength of weak ties is dependent on what exactly is being diffused in the network. They showed that when considering complex contagion, where an individual needs to be exposed to multiple sources before being “activated” themselves, weak ties can lower diffusion. Whereas certain types of information or diseases can be passed on by a single moment of contact (simple contagion), more complex behaviours or beliefs may only be adopted once an individual sees that multiple of its connections have done so. As a result, on the same network, the pattern and size of diffusion differs depending on whether its spread is governed by simple or complex contagion.

\subsubsection{Multiplex networks}
The literature discussed so far concerns properties of a single network. However, individuals seldomly partake in only one network. Multiple networks that are related to each other can be described through multiplex networks. Multiplex networks are networks where nodes are linked to each other with more than one type of edge, and the edges of one type that connect nodes together form one layer \citep{Lee2015}. Multiplex networks are of interest because interaction between the different layers can create non-additive and non-linear effects on the overall network. This is also why it is important to use multiplex networks when exploring systems that include multiple types of connections in reality – if the corresponding network model only has one layer, it may not correctly describe the behaviour of the system it should represent \citep{Lee2015}. \\

The general network properties discussed in the introduction also apply – with some modification – to multiplex network. An additional property of interest in multiplex networks is the correlation between different layers. Correlation can be described in multiple ways, ranging from the extent of node multiplicity (i.e. how many vertices appear in different layers), to interlayer degree correlation, edge overlap, and cross-layered clustering coefficients \citep{Lee2015}. \\

In terms of diffusion over networks, it has been shown by \cite{Brummitt2012} in a case of threshold activation (a form of complex contagion, i.e. x number of neighbours need to be activated for a node to be activated), multiplex networks can show cascade effects even if the individual layers are not susceptible to global cascades. This shows that when studying cascade effects, it is necessary to consider any multiplicity in the system of interest. The same authors later showed that heterogeneity in threshold activation rules can enhance or inhibit cascades, depending on how many nodes require the threshold to be met in all layers \citep{Lee2014}. \cite{Sahneh2014}) showed that, when considering the diffusion of competing and exclusive viruses, in a multiplex network there exists an equilibrium in which the two viruses to coexist simultaneously. In a single-layer network this is not possible: one of the two will always come to fully dominate the other. \\

\cite{Yagan2012} consider the case where the influence of two types of connections is weighted (i.e. the influence of a neighbour in one network is greater than that in the other) and analyse how this influences complex (threshold) contagion.  Their result show that both average degree and the relative difference in weighted influence strongly affect the probability of a global cascade, as well as its subsequent size. \\

The fact that network multiplicity has such a strong effect on probability and size of cascades has substantial consequences for studying the effect of disinformation. Most studies on the spread of disinformation (or closely related phenomena) (these will be discussed in detail in the following section), model the spread of beliefs over a single layer network. However, individuals gain information from multiple sources and in the case of online disinformation, are likely exposed to mediating (or amplifying) influences from their offline (“real”) network of family and friends. Therefore, I propose that in order to properly study the spread of disinformation over a network, one needs to consider a multiplex network of at least two layers representing the online network and the offline network. 


\subsection{Psychology of advertising}
\cite{Fennis2015} define advertising as “any form of paid communication by an identified sponsor aimed to inform and/or persuade target audiences about an organization, product, service or idea.” Until the 20th century, advertisements focussed on the use of information or arguments to convince potential customers. In the early 1900s, advertisements also began using emotional appeals and the projection of ideas and beliefs. Both approaches have coexisted since then. \\

The psychology of advertising aims to identify how (characteristics) of advertisements affect individuals and to understand the psychological processes behind those effects \citep{Fennis2015}. The field was pioneered by \cite{Scott1916} with his book “The Psychology of Advertising”. Generally, three types of outcomes are studied: cognitive responses (beliefs and thoughts about a brand), affective responses (moods and emotions), and behavioural responses (buying a product, switching to a different brand) \citep{Fennis2015}. \\

Advertisers are mainly interested in changing the attitudes (which are made up of both cognitive beliefs as well as the affective state) of potential customers, as well as ensuring that a certain attitude leads to the desired behaviour. There are several theories on how attitudes change. One of the earliest was the Yale Attitude Change approach, developed by \cite{Hovland1953}, which focused on factors related to the source, the form of communication, and the audience. They also proposed several stages in which a message was processed, a framework that was extended in the Information Processing Model of \cite{McGuire1968}. He suggested six stages in which a message is processed: presentation, attention, comprehension, yielding, retention and behaviour. The stage of yielding is where attitude change occurs. Both the Yale Attitude Change model and the Information Processing Model assume that this is an active and conscious process. A different theory was proposed by \cite{Greenwald1968} in his Cognitive Response Model. He shifted away from models of learning and memorization of new information and stated that persuasion depended mainly on whether someone accepts the premise of the message when viewing it. \\

Dual process theories were developed to address the fact that the variables suggested by earlier models could not consistently explain the response to a message \cite{Xu2017}. The two prominent dual process models are the Elaboration Likelihood Model by Petty \& Cacioppo (1986) and the Heuristic Systematic Model by Chaiken et al (1980). In general these models assume that persuasion can happen in two ways: The first is through a conscious process in which the recipient engages with new information and adjusts their beliefs accordingly. The second route is reliant on heuristics and is used when someone’s motivation or skills are too low to process the information fully. Building on the dual process theories, \cite{Kruglanski1999} proposed the unimodel, suggesting that the two types of processing were not separate  processes, but variations of the same process that could be triggered by different contextual factors. \\

Advertisers are not only interested in convincing customers that their product is good, but hope that this is also translated in the behaviour of the customer. For a long time, attitude was seen as the most important predictor of behaviour. However, empirical studies showed that it could not be the sole explanation \citep{Fennis2015}. Fishbein and Ajzen formulated the Theory of Reasoned Action in the 1970s, which also took “subjective norms” into account (which describe social norms and the desire to comply with those norms by (not) performing a certain behaviour). They later extended this into the Theory of Planned Behaviour, which added the “perceived behavioural control” individuals had over the action they may perform (Ajzen, 1991). The three elements (attitude, subjective norm, and perceived behavioural control) predict the intention to perform a certain behaviour well, but there still remains a gap between intention and performing the behaviour itself \citep{Fennis2015}. \\

This theory, again, is based on a conscious process in which an intention is formed. Yet advertisers may also be interested in processes where certain behaviour is performed automatically.  The simplest form of this is habitual behaviour, where past behaviour is a better predictor than someone’s intention. However, advertisers may also try to prime behaviours to meet some goal (i.e. unconsciously smelling cleaning product might lead someone to formulate the goal of cleaning their house) \citep{Fennis2015}.  


\subsection{Propaganda} 
%could this be absorbed in the disinfo societal effects section?

\subsection{Simulation modelling}
Agent-based modelling is a method that allows for the combination of many factors and mechanism that make up a complex system in order to study their interaction and resulting behaviour \citep{Flache2017}.  In the social sciences it is a useful approach precisely because it allows for the combination of different disciplines in order to mimic some of the multidimensionality of the real world \citep{Epstein2006}. \\

\cite{Nowak1990} were among the first to suggest that individual psychological processes proposed in theory could be simulated in order to see if they produced the expected group behaviour. They simulated Social Impact Theory, which assumes that social influence is caused by the strength of sources, the immediacy of sources, and the number of sources supporting a certain attitude. They modelled binary opinion change on a gridded population, and showed that clusters of minority opinions could be maintained. \\

\cite{Deffuant2002} studied how extreme opinions can prevail in communities. They modelled a population in which individuals have random 1-to-1 meetings, and in such a meeting the opinion of either individual changes based on the opinion and uncertainty of the other. Extremists were modelled as having both extreme opinions and confidence (low uncertainty). If the general population has high uncertainty, extremists are able to polarise a population (i.e. bipolar extremism), or have the population convert to either extreme standpoint. \\

\cite{Flache2017} reviewed a large set of agent-based models on social influence. They showed that depending on what assumption is made on how individuals influence each other, the overall pattern of system behaviour changes significantly – from consensus to fragmented clusters of opinions to polarization. They stress that there are many models describing social influence, but that comparison between models is lacking, as well as empirical testing. \\

\cite{Ross2019} used agent-based modelling to study the phenomenon of a “Spiral of Silence”, in which people with the minority opinion do not dare speak up, causing a positive feedback loop. In particular, they studied how bots could influence opinion formation by triggering a spiral of silence. They found that bots could do so, even if their influence was a fourth of that of real humans. The denser the network, the stronger this effect. \\

(Battiston, Cairoli, Nicosia, Baule, \& Latora, 2016)
Chattoe-Brown, E. (2014). Using agent based modelling to integrate data on attitude change \\

Notably, many of these studies tend to isolate the phenomena of interests. For example, \cite{Deffuant2002} considered several mechanisms for how two individuals exchange opinions, but leave out of scope how, why or when individuals meet to do this. On the flipside, \cite{Ross2019} only consider the influence of someone’s online connections in their decision to speak up, whereas this decision is likely influenced by many social and political factors. Such scoping decisions are understandable from the perspective of wanting to isolate mechanisms behind a phenomena, but make it hard to translate the results to the messier real world.
